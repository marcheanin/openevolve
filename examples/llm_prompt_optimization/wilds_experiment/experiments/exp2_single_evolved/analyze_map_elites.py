#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ MAP-Elites —Å–µ—Ç–∫–∏ –¥–ª—è exp2_single_evolved.
–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞, —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ä–µ—à–µ–Ω–∏–π –∏ –∫–∞—á–µ—Å—Ç–≤–æ –≤ —Ä–∞–∑–Ω—ã—Ö –Ω–∏—à–∞—Ö.
"""

import json
import sys
import os
from pathlib import Path
from collections import defaultdict
from typing import Dict, List, Tuple, Any
import numpy as np

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ UTF-8 –¥–ª—è –≤—ã–≤–æ–¥–∞
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

def load_evolution_trace(trace_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å evolution trace –∏–∑ JSONL —Ñ–∞–π–ª–∞."""
    traces = []
    if not Path(trace_path).exists():
        print(f"Error: Evolution trace not found at {trace_path}")
        return traces
    
    with open(trace_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                try:
                    trace = json.loads(line)
                    traces.append(trace)
                except json.JSONDecodeError as e:
                    print(f"Warning: Could not parse line: {e}")
                    continue
    
    return traces

def extract_feature_data(traces: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ –æ feature dimensions –∏ scores."""
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ (criteria_explicitness, domain_focus)
    # MAP-Elites —Ö—Ä–∞–Ω–∏—Ç –ª—É—á—à–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–∞–∂–¥–æ–π –Ω–∏—à–∏
    niche_best = {}  # (criteria_bin, domain_bin) -> best score
    niche_data = defaultdict(list)  # (criteria_bin, domain_bin) -> list of scores
    
    # –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    all_criteria = []
    all_domain = []
    all_scores = []
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–µ—Ç–∫–∏ (–∏–∑ config.yaml)
    feature_bins = 10  # 10x10 —Å–µ—Ç–∫–∞ = 100 –Ω–∏—à
    
    for trace in traces:
        child_metrics = trace.get("child_metrics", {})
        if not child_metrics:
            continue
        
        criteria = child_metrics.get("criteria_explicitness")
        domain = child_metrics.get("domain_focus")
        score = child_metrics.get("combined_score", 0.0)
        
        if criteria is None or domain is None:
            continue
        
        all_criteria.append(criteria)
        all_domain.append(domain)
        all_scores.append(score)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º bin –¥–ª—è MAP-Elites (0-9 –¥–ª—è –∫–∞–∂–¥–æ–π –æ—Å–∏)
        criteria_bin = min(int(criteria * feature_bins), feature_bins - 1)
        domain_bin = min(int(domain * feature_bins), feature_bins - 1)
        niche_key = (criteria_bin, domain_bin)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–∏–π score –¥–ª—è —ç—Ç–æ–π –Ω–∏—à–∏
        if niche_key not in niche_best or score > niche_best[niche_key]:
            niche_best[niche_key] = score
        
        niche_data[niche_key].append(score)
    
    return {
        "niche_best": niche_best,
        "niche_data": dict(niche_data),
        "all_criteria": all_criteria,
        "all_domain": all_domain,
        "all_scores": all_scores,
        "feature_bins": feature_bins,
    }

def analyze_coverage(data: Dict[str, Any]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞."""
    niche_best = data["niche_best"]
    feature_bins = data["feature_bins"]
    total_niches = feature_bins * feature_bins
    
    filled_niches = len(niche_best)
    coverage_percent = (filled_niches / total_niches) * 100
    
    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –æ—Å—è–º
    criteria_bins_filled = set()
    domain_bins_filled = set()
    
    for criteria_bin, domain_bin in niche_best.keys():
        criteria_bins_filled.add(criteria_bin)
        domain_bins_filled.add(domain_bin)
    
    criteria_coverage = (len(criteria_bins_filled) / feature_bins) * 100
    domain_coverage = (len(domain_bins_filled) / feature_bins) * 100
    
    return {
        "total_niches": total_niches,
        "filled_niches": filled_niches,
        "coverage_percent": coverage_percent,
        "criteria_bins_filled": len(criteria_bins_filled),
        "domain_bins_filled": len(domain_bins_filled),
        "criteria_coverage_percent": criteria_coverage,
        "domain_coverage_percent": domain_coverage,
    }

def analyze_quality_distribution(data: Dict[str, Any]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–æ –Ω–∏—à–∞–º."""
    niche_best = data["niche_best"]
    all_scores = data["all_scores"]
    
    if not niche_best:
        return {}
    
    niche_scores = list(niche_best.values())
    
    return {
        "best_score_overall": max(niche_scores) if niche_scores else 0.0,
        "worst_score_overall": min(niche_scores) if niche_scores else 0.0,
        "mean_score": np.mean(niche_scores) if niche_scores else 0.0,
        "median_score": np.median(niche_scores) if niche_scores else 0.0,
        "std_score": np.std(niche_scores) if niche_scores else 0.0,
        "mean_all_evaluations": np.mean(all_scores) if all_scores else 0.0,
        "num_evaluations": len(all_scores),
    }

def analyze_feature_distribution(data: Dict[str, Any]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ feature dimensions."""
    all_criteria = data["all_criteria"]
    all_domain = data["all_domain"]
    
    if not all_criteria or not all_domain:
        return {}
    
    return {
        "criteria": {
            "min": min(all_criteria),
            "max": max(all_criteria),
            "mean": np.mean(all_criteria),
            "median": np.median(all_criteria),
            "std": np.std(all_criteria),
        },
        "domain": {
            "min": min(all_domain),
            "max": max(all_domain),
            "mean": np.mean(all_domain),
            "median": np.median(all_domain),
            "std": np.std(all_domain),
        },
    }

def find_best_niches(data: Dict[str, Any], top_n: int = 5) -> List[Tuple[Tuple[int, int], float]]:
    """–ù–∞–π—Ç–∏ –ª—É—á—à–∏–µ –Ω–∏—à–∏ –ø–æ combined_score."""
    niche_best = data["niche_best"]
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
    sorted_niches = sorted(niche_best.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_niches[:top_n]

def generate_heatmap_data(data: Dict[str, Any]) -> np.ndarray:
    """–°–æ–∑–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è heatmap (10x10 –º–∞—Ç—Ä–∏—Ü–∞)."""
    niche_best = data["niche_best"]
    feature_bins = data["feature_bins"]
    
    # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É, –∑–∞–ø–æ–ª–Ω—è–µ–º NaN –¥–ª—è –ø—É—Å—Ç—ã—Ö –Ω–∏—à
    heatmap = np.full((feature_bins, feature_bins), np.nan)
    
    for (criteria_bin, domain_bin), score in niche_best.items():
        heatmap[criteria_bin, domain_bin] = score
    
    return heatmap

def print_analysis_report(data: Dict[str, Any], coverage: Dict[str, Any], 
                         quality: Dict[str, Any], features: Dict[str, Any],
                         best_niches: List[Tuple[Tuple[int, int], float]]):
    """–í—ã–≤–µ—Å—Ç–∏ –æ—Ç—á–µ—Ç –æ–± –∞–Ω–∞–ª–∏–∑–µ."""
    print("=" * 80)
    print("MAP-ELITES –°–ï–¢–ö–ê: –ê–ù–ê–õ–ò–ó –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò")
    print("=" * 80)
    print()
    
    print("üìä –ü–û–ö–†–´–¢–ò–ï –ü–†–û–°–¢–†–ê–ù–°–¢–í–ê")
    print("-" * 80)
    print(f"–í—Å–µ–≥–æ –Ω–∏—à –≤ —Å–µ—Ç–∫–µ: {coverage['total_niches']} (10x10)")
    print(f"–ó–∞–ø–æ–ª–Ω–µ–Ω–æ –Ω–∏—à: {coverage['filled_niches']}")
    print(f"–ü–æ–∫—Ä—ã—Ç–∏–µ: {coverage['coverage_percent']:.1f}%")
    print()
    print(f"–ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º (criteria_explicitness):")
    print(f"  –ó–∞–ø–æ–ª–Ω–µ–Ω–æ bins: {coverage['criteria_bins_filled']}/{coverage['total_niches']//10}")
    print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ: {coverage['criteria_coverage_percent']:.1f}%")
    print()
    print(f"–ü–æ–∫—Ä—ã—Ç–∏–µ –ø–æ –¥–æ–º–µ–Ω—É (domain_focus):")
    print(f"  –ó–∞–ø–æ–ª–Ω–µ–Ω–æ bins: {coverage['domain_bins_filled']}/{coverage['total_niches']//10}")
    print(f"  –ü–æ–∫—Ä—ã—Ç–∏–µ: {coverage['domain_coverage_percent']:.1f}%")
    print()
    
    print("üéØ –ö–ê–ß–ï–°–¢–í–û –†–ï–®–ï–ù–ò–ô")
    print("-" * 80)
    print(f"–õ—É—á—à–∏–π combined_score: {quality['best_score_overall']:.4f}")
    print(f"–•—É–¥—à–∏–π combined_score: {quality['worst_score_overall']:.4f}")
    print(f"–°—Ä–µ–¥–Ω–∏–π combined_score (–ø–æ –Ω–∏—à–∞–º): {quality['mean_score']:.4f}")
    print(f"–ú–µ–¥–∏–∞–Ω–Ω—ã–π combined_score: {quality['median_score']:.4f}")
    print(f"–°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {quality['std_score']:.4f}")
    print()
    print(f"–í—Å–µ–≥–æ –æ—Ü–µ–Ω–æ–∫: {quality['num_evaluations']}")
    print(f"–°—Ä–µ–¥–Ω–∏–π score –≤—Å–µ—Ö –æ—Ü–µ–Ω–æ–∫: {quality['mean_all_evaluations']:.4f}")
    print()
    
    print("üìà –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û FEATURE DIMENSIONS")
    print("-" * 80)
    if features:
        print("criteria_explicitness:")
        print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: [{features['criteria']['min']:.3f}, {features['criteria']['max']:.3f}]")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ: {features['criteria']['mean']:.3f}")
        print(f"  –ú–µ–¥–∏–∞–Ω–∞: {features['criteria']['median']:.3f}")
        print(f"  –°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {features['criteria']['std']:.3f}")
        print()
        print("domain_focus:")
        print(f"  –î–∏–∞–ø–∞–∑–æ–Ω: [{features['domain']['min']:.3f}, {features['domain']['max']:.3f}]")
        print(f"  –°—Ä–µ–¥–Ω–µ–µ: {features['domain']['mean']:.3f}")
        print(f"  –ú–µ–¥–∏–∞–Ω–∞: {features['domain']['median']:.3f}")
        print(f"  –°—Ç–¥. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {features['domain']['std']:.3f}")
    print()
    
    print("üèÜ –¢–û–ü-5 –õ–£–ß–®–ò–• –ù–ò–®")
    print("-" * 80)
    for i, ((criteria_bin, domain_bin), score) in enumerate(best_niches, 1):
        criteria_val = criteria_bin / 10.0
        domain_val = domain_bin / 10.0
        print(f"{i}. –ù–∏—à–∞ ({criteria_bin}, {domain_bin}) "
              f"[criteria={criteria_val:.1f}, domain={domain_val:.1f}]: "
              f"score={score:.4f}")
    print()
    
    print("=" * 80)
    print("–û–¶–ï–ù–ö–ê –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò")
    print("=" * 80)
    print()
    
    # –û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
    coverage_score = coverage['coverage_percent'] / 100.0
    quality_score = quality['best_score_overall']  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1 (score —É–∂–µ –≤ —ç—Ç–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ)
    diversity_score = features['criteria']['std'] * features['domain']['std'] if features else 0.0
    
    print(f"–ü–æ–∫—Ä—ã—Ç–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞: {coverage_score*100:.1f}% "
          f"{'‚úÖ –û—Ç–ª–∏—á–Ω–æ' if coverage_score > 0.5 else '‚ö†Ô∏è –ù–∏–∑–∫–æ–µ' if coverage_score < 0.2 else '‚úÖ –•–æ—Ä–æ—à–æ'}")
    print(f"–ö–∞—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π: {quality['best_score_overall']:.4f} "
          f"{'‚úÖ –û—Ç–ª–∏—á–Ω–æ' if quality['best_score_overall'] > 0.6 else '‚ö†Ô∏è –ù–∏–∑–∫–æ–µ' if quality['best_score_overall'] < 0.4 else '‚úÖ –•–æ—Ä–æ—à–æ'}")
    print(f"–†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ: std(criteria)={features['criteria']['std']:.3f}, "
          f"std(domain)={features['domain']['std']:.3f} "
          f"{'‚úÖ –•–æ—Ä–æ—à–æ' if features['criteria']['std'] > 0.2 and features['domain']['std'] > 0.2 else '‚ö†Ô∏è –ù–∏–∑–∫–æ–µ'}")
    print()
    
    # –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞
    if coverage_score > 0.3 and quality['best_score_overall'] > 0.5:
        print("‚úÖ –°–ï–¢–ö–ê –≠–§–§–ï–ö–¢–ò–í–ù–ê: –•–æ—Ä–æ—à–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ—à–µ–Ω–∏–π")
    elif coverage_score > 0.5:
        print("‚ö†Ô∏è –°–ï–¢–ö–ê –ß–ê–°–¢–ò–ß–ù–û –≠–§–§–ï–ö–¢–ò–í–ù–ê: –•–æ—Ä–æ—à–µ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å")
    elif quality['best_score_overall'] > 0.5:
        print("‚ö†Ô∏è –°–ï–¢–ö–ê –ß–ê–°–¢–ò–ß–ù–û –≠–§–§–ï–ö–¢–ò–í–ù–ê: –•–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –Ω–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ")
    else:
        print("‚ùå –°–ï–¢–ö–ê –ù–ï–≠–§–§–ï–ö–¢–ò–í–ù–ê: –ù–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ –∏/–∏–ª–∏ –∫–∞—á–µ—Å—Ç–≤–æ")
    print()

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    exp_dir = Path(__file__).parent
    trace_path = exp_dir / "openevolve_output" / "evolution_trace.jsonl"
    
    if not trace_path.exists():
        print(f"Error: Evolution trace not found at {trace_path}")
        sys.exit(1)
    
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    traces = load_evolution_trace(str(trace_path))
    
    if not traces:
        print("No traces found. Cannot analyze.")
        sys.exit(1)
    
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(traces)} –∑–∞–ø–∏—Å–µ–π –∏–∑ evolution trace")
    print()
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ feature dimensions...")
    data = extract_feature_data(traces)
    
    if not data["niche_best"]:
        print("No feature dimension data found in traces.")
        sys.exit(1)
    
    # –ê–Ω–∞–ª–∏–∑
    print("–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞...")
    coverage = analyze_coverage(data)
    
    print("–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ—à–µ–Ω–∏–π...")
    quality = analyze_quality_distribution(data)
    
    print("–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ feature dimensions...")
    features = analyze_feature_distribution(data)
    
    print("–ü–æ–∏—Å–∫ –ª—É—á—à–∏—Ö –Ω–∏—à...")
    best_niches = find_best_niches(data, top_n=5)
    
    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    print_analysis_report(data, coverage, quality, features, best_niches)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_path = exp_dir / "openevolve_output" / "map_elites_analysis.json"
    results = {
        "coverage": coverage,
        "quality": quality,
        "features": features,
        "best_niches": [
            {
                "niche": list(niche),
                "criteria_bin": niche[0],
                "domain_bin": niche[1],
                "criteria_value": niche[0] / 10.0,
                "domain_value": niche[1] / 10.0,
                "score": score
            }
            for niche, score in best_niches
        ],
        "total_niches_filled": len(data["niche_best"]),
        "total_evaluations": len(data["all_scores"]),
    }
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")

if __name__ == "__main__":
    main()
