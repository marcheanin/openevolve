max_iterations: 100
checkpoint_interval: 10
log_level: "INFO"
diff_based_evolution: false
max_code_length: 10000
language: "text"

llm:
  api_base: "https://llm.api.cloud.yandex.net/v1"
  models:
    - name: "gpt://b1gemincl8p7b2uiv5nl/qwen3-235b-a22b-fp8/latest"
      weight: 1.0
  temperature: 0.8
  max_tokens: 4096
  timeout: 60
  retries: 3

prompt:
  template_dir: "../../templates"
  num_top_programs: 5
  num_diverse_programs: 3
  include_artifacts: true
  system_message: |
    You are an expert at creating effective prompts for ensemble sentiment classification.
    Your goal is to evolve prompts that maximize accuracy for Home and Kitchen
    product reviews (ratings 1-5) when using a 3-model voting ensemble.

    ## Task Overview
    The evaluator receives product reviews and must classify them into star
    ratings (1-5). Your evolved prompts will be used by three LLM workers
    (Qwen3-235B, Gemma3-27B, GPT-OSS-120B). The final prediction is the
    majority vote of the three workers.

    ## How Evaluation Works
    When your prompt is evaluated:
    1. The evaluator receives a product review text (example below)
    2. It replaces the {review} placeholder in your prompt with the actual review text
    3. It sends the formatted prompt to each worker model
    4. Each worker responds with a rating (1-5)
    5. The majority vote becomes the final prediction
    6. The evaluator compares this prediction to the true gold label

    ### Example Input to Evaluator
    **Review text example:**
    "This coffee maker is amazing! Works perfectly, makes great coffee every
    morning, and looks great in my kitchen. The automatic timer feature is
    really convenient. I would definitely recommend this to anyone looking for
    a reliable coffee maker."

    **Gold label (true rating):** 5

    The evaluator would format your prompt like this:
    ```
    [Your prompt text here]
    Review: This coffee maker is amazing! Works perfectly, makes great coffee
    every morning, and looks great in my kitchen. The automatic timer feature
    is really convenient. I would definitely recommend this to anyone looking
    for a reliable coffee maker.
    Star Rating (respond with only the number 1, 2, 3, 4, or 5):
    ```

    Each LLM should respond with just the number (e.g., "5").

    ## Metrics Explained
    ### Primary Metrics
    - **R_global** (Global Accuracy): Overall accuracy. Higher is better.
    - **R_worst** (Worst-User Accuracy): 10th percentile of per-user accuracy.
      Measures fairness - how well the ensemble works for worst-performing users.
    - **MAE** (Mean Absolute Error): Average absolute difference between
      predicted and true ratings. Range: 0.0 to 4.0. Lower is better.
    - **Cohen's Kappa (κ)**: Inter-annotator agreement between ensemble
      workers, corrected for chance. Range: [-1, 1] (we use max(0, κ) in
      scoring). Higher κ = better consensus; κ=0 means no better than random.

    ### Combined Score Formula
    The **combined_score** is the primary metric used for evolution.
    It uses a unified formula that balances accuracy, fairness, precision,
    and ensemble agreement (Cohen's Kappa):

    ```
    base_score = 0.4 * R_global + 0.3 * R_worst + 0.3 * (1 - MAE/4)
    kappa_score = max(0, mean_kappa)   # clip to [0, 1]
    consistency_bonus = 0.1 * kappa_score
    combined_score = base_score + consistency_bonus
    ```

    Where:
    - `R_global`: Global accuracy on validation set (40% weight)
    - `R_worst`: 10th percentile per-user accuracy on validation set (30% weight)
    - `MAE`: Mean Absolute Error on validation set (30% weight)
    - `mean_kappa`: Cohen's Kappa (pairwise, averaged); clipped to [0, 1] (10% bonus)
    - `MAE/4` normalizes MAE to 0-1 range (since max MAE = 4.0)
    - Kappa penalizes "always predict majority" and rewards real agreement

    **Example calculation:**
    - Validation: R_global = 0.65, R_worst = 0.44, MAE = 0.36, κ = 0.50
    
    base_score = 0.4 * 0.65 + 0.3 * 0.44 + 0.3 * (1 - 0.36/4)
               = 0.26 + 0.132 + 0.273 = 0.665
    consistency_bonus = 0.1 * max(0, 0.50) = 0.05
    combined_score = 0.665 + 0.05 = 0.715

    **Note:** Kappa is stricter than disagreement rate: it punishes ensembles
    that simply agree on the majority class without real discriminative agreement.

    ## Best Practices
    1. Use clear, unambiguous instructions
    2. Provide specific criteria for each rating level (1-5)
    3. Consider including 1–2 few-shot examples (review + star rating) from the training
       distribution; they help anchor the format and clarify the task for the workers.
    4. Avoid overfitting to specific review patterns
    5. Focus on generalization: prompts should work well on both train and validation sets
    6. Keep prompts concise but informative
    7. Ensure the prompt explicitly requests only a number (1-5) as output

    ## MAP-Elites Feature Dimensions
    The evolution uses MAP-Elites to maintain diversity across two dimensions:

    1. **sentiment_vocabulary_richness** (0.0 - 1.0):
       - Combined metric measuring both sentiment vocabulary and example richness
       - Combines: sentiment vocabulary (60% weight) + example richness (40% weight)
       - Higher = more sentiment words (love, hate, excellent, terrible, etc.) AND more examples
       - Lower = fewer sentiment words and fewer examples
       - **Tip**: Include sentiment words like "excellent", "terrible", "satisfied" and examples like "for example", "such as", quoted phrases

    2. **mean_kappa** (0.0 - 1.0, from Cohen's κ clipped to [0, 1]):
       - Inter-annotator agreement between workers (Landis & Koch scale)
       - Higher = substantial/almost perfect agreement; κ=0 = no better than random
       - **Goal**: Find prompts that yield real discriminative agreement, not just "always majority"

    The algorithm explores diverse prompts across this 2D space:
    - Low κ: prompts where workers agree no better than chance (or worse)
    - High κ: prompts with substantial agreement (ideal for reliability)

database:
  population_size: 50
  archive_size: 500
  num_islands: 4
  feature_dimensions: ["sentiment_vocabulary_richness", "mean_kappa"]
  feature_bins: 10
  elite_selection_ratio: 0.4
  exploration_ratio: 0.3
  exploitation_ratio: 0.3
  migration_interval: 20
  migration_rate: 0.1

evaluator:
  timeout: 1800
  max_retries: 3
  parallel_evaluations: 4
  cascade_evaluation: false

generalization:
  gap_threshold: 0.10
  gap_penalty: 0.50

evolution_trace:
  enabled: true
  format: "jsonl"
  include_code: true
  include_prompts: true
  output_path: "openevolve_output/evolution_trace.jsonl"
  buffer_size: 10
  compress: false

worker_defaults:
  api_base: "https://llm.api.cloud.yandex.net/v1"
  temperature: 0.1
  max_tokens: 64
  timeout: 60
  max_retries: 3

workers:
  - name: "yandexgpt"
  - name: "gemma3-27b"
  - name: "gpt-oss-120b"

dataset:
  config_path: "dataset.yaml"
  max_samples: null
  max_samples_train: null
  max_samples_val: null
  max_samples_test: null
  max_train_users: 10
  max_val_users: 15

prompt_path: "initial_prompt.txt"
output_dir: "results"


