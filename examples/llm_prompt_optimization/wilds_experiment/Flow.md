
Здесь основной текст того, как я вижу этот доклад.

## Мотивация 

#### Введение

Исходная работа была сосредоточена на исследовании способностей алгоритма AlphaEvolve (в открытой реализации OpenEvolve) к оптимизации промптов на разнородных задачах:
- Многошаговое рассуждение (HotpotQA)
- Следование инструкциям (ifEval)
- Классификация текстов с распределенным сдвигом (WILDS Amazon)
Эксперименты продемонстрировали как потенциал эволюционного поиска в пространстве дискретных инструкций, так и его ограничения.

В частности:

- HotpotQA наблюдалось небольшое улучшение при уже высоком baseline, с подозрением на утечку в предобучение модели
- На ifEval проведено множество вариаций экспериментов (разные размеры выборок, стратегии отбора примеров, разные вариации расчета метрик), показавших, что эффективность сильно зависит от мощности как evaluator, так и sampler моделей; однако никаких драматических изменений достигнуто не было
- на WILDS Amazon попытки поощрить few-shot примеры, включить валидационные метрики и другие подходы дали лишь незначительные улучшения к baseline (0.75 -> 0.77 test accuracy, 0.74 eval accuracy max)

Эти результаты указывают на необходимость переформулирования целевой задачи. Вместо общей оценки возможностей AlphaEvolve на разнородных бенчмарках и разных типах задач, пробуем переходить к новой, практически значимой проблеме, где эволюционный поиск инструкций показывает наибольший потенциал. Таковой является задача **автоматической разметки данных и оптимизации инструкций для множества аннотаторов**.

#### 1. Критическая проблема разметки данных в современном машинном обучении

##### 1.1 Разметка как узкое место в цикле разработки

Несмотря на впечатляющий прогресс в развитии больших языковых моделей, процесс создания качественных размеченных датасетов остается одной из самых затратных, медленных и критических операций в цикле разработки AI-систем. Это актуально как для академических исследований, так и для индустрии:

- **Стоимость:** привлечение человеческих экспертов обходится дорого; crowdsourcing требует управления, контроля качества и переразметки
- **Масштабируемость**: для современных моделей требуются датасеты из десятков или сотен тысяч примеров; человеческая разметка не масштабируется линейно с ростом объема данных
- **Качество**: даже при четких инструкциях inter-annotation agreement редко превышает 0.70-0.85, а несогласия требуют дополнительного разбора или переразметки 
- **Консистентность:** единые критерии разметки со временем дрейфуют (разные аннотаторы в разные дни дают разные метки на похожих примерах)

```
IAA (Inter‑Annotator Agreement) — это степень согласия между несколькими аннотаторами, которые размечают одни и те же объекты для одной и той же задачи.

Обычно под IAA понимают не одну метрику, а **класс показателей**, которые измеряют, насколько разметчики ставят одинаковые метки и насколько это выше случайного совпадения:​

- Cohen’s κ — для двух аннотаторов, категориальные метки, корректирует на случайное совпадение.​
- Fleiss’ κ, Krippendorff’s α — для многих аннотаторов, могут работать с пропусками, порядковыми шкалами и т.п.
```

Эта проблема была фундаментально изучена в контексте crowdsourcing. Классическая модель Dawid-Skene (1979) и её развития показали, что в условиях множественной разметки с разнокачественными аннотаторами основная задача - это агрегировать противоречивые метки и оценить надежность каждого аннотатора. Однако сам процесс выработки качественных инструкций оставался экспертной, неформальной практикой. [[Оценки противоречивых судей]] - тут описал как это развивалось и какие есть современные работы.

##### 1.2 LLM как решение и источник новых вызовов

С появлением больших языковых моделей возникла надежда использовать их как автоматических, масштабируемых аннотаторов. Zero-shot классификация LLM, основанная на prompt-based, позволяет:

- избежать затрат на human annotation
- получить ответ за секунды на любое кол-во примеров
- адаптироваться к новым задачам без fine-tuning

Однако, как показано в работе  [[Статья Majority Rules - LLM Ensemble is a Winning Approach for Content Categorization]] (в конце), одиночные LLM демонстрируют серьезные ограничения в задачах таксономической классификации (классификация по фиксированному дереву категорий):

- Плато качества: увеличение размера модели от 8B к 120B параметров НЕ приводит к пропорциональному улучшению (F1 остается в диапазоне 0.50-0.55)
- Hallucination ratio (HR): модели часто выдают категории, вообще не входящие в таксономию
- Category Inflation Ratio (IR): модели систематически раздувают число предсказанных категорий (выдают 4-5 вместо 1-2 по мнению экспертов)

Эти проблемы возникают не из-за недостатка параметров, а из-за структурной природы задачи: таксонометрическая классификация требует жесткой компрессии богатого семантического содержания в малое, разреженное множество фиксированных категорий. Такие задачи сложны даже для самых сложных моделей.

##### 1.3 История оценок в NLP: от автоматических метрик к LLM-судьям

Развитие методов автоматической оценки качества текстовых систем показывают логичную эволюцию от поверхностных метрик к глубокому пониманию смысла. Эта история объясняет, почему LLM-как-судья - это не просто новая идея, а необходимый шаг в развитии оценочных методов.

**Эпоха 1: Автоматические метрики (BLEU, ROUGE, METEOR)**

Классические автоматические метрики строятся на идее сравнения с эталонным текстом по n-граммам и близким формальным критериям. Тут я привел подробный обзор что это за метрики [[Метрики оценки качества ответов]]

Долгие годы оценка качества NLP систем полагалась на **автоматические метрики**:​

- **BLEU (1995)**: для машинного перевода, основана на совпадении n-грамм между выходом и эталоном. Простота и скорость расчёта делали её стандартом в исследованиях.
- **ROUGE (2003)**: для суммаризации, основана на recall и longest common subsequences.
- **METEOR (2005)**: улучшение BLEU, учитывает синонимы, stems и паraphrase matching.

Эти метрики были **простыми, быстрыми и воспроизводимыми**, позволяя научному сообществу проводить стандартизованное сравнение систем. Однако их ограничения были **фундаментальными**.

Работа **"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Metrics for Machine Translation"** (ACL 2020) показала парадокс, который долгое время игнорировался:[](https://aclanthology.org/2020.acl-main.448.pdf)​

> BLEU хорошо коррелирует с человеческими оценками только при **больших различиях между системами** (>5 пунктов). При малых различиях (0–3 пункта) BLEU **часто не совпадает** с человеческим мнением.

Это привело к двум типам систематических ошибок:

- **Type I ошибка**: принятие незначимых улучшений (BLEU +1–2 пункта, но люди не видят реальной разницы в качестве).
- **Type II ошибка**: отвержение реальных улучшений (люди явно видят улучшение, но BLEU не поднялся значимо).

В контексте разработки: исследователь может вложить недели работы в оптимизацию, получить улучшение на 2 пункта BLEU, но это улучшение **статистически незначимо с точки зрения людей**. Обратная ситуация ещё хуже: изменение может улучшить качество с точки зрения людей, но метрика его не зафиксирует.

Ещё более острая проблема, выявленная в работе **"When AI Scores High but Fails with Humans: Rethinking Evaluation Metrics"** (2005):[](https://www.zentao.pm/blog/when-ai-scores-high-but-fails-with-humans-rethinking-evaluation-metrics-1725.html)​

> Модель может получить высокие BLEU/ROUGE баллы, но люди описывают её выход как "холодный, робото, без подлинности" и отсутствия эмоциональной теплоты.

**Причина**: автоматические метрики оптимизируют **структурную точность** — синтаксис, правописание, граммар. Но они **не видят**:

- Контекста и релевантности ответа вопросу
- Тона и эмоционального резонанса
- Креативности и естественности языка
- Полезности и praktical value для пользователя

Модель может сгенерировать синтаксически идеальный ответ, который получит высокий BLEU, но будет бесполезен для человека из-за холодности и отсутствия релевантности к контексту.

**Эпоха 2: Нейронные метрики (BERTScore, BLEURT)**

Попытка преодолеть эти ограничения привела ко второй волне — **нейронным метрикам**:​

- **BERTScore** (2020): вместо совпадения n-грамм сравнивает **контекстные эмбеддинги** из BERT. Это позволяет ловить **синонимичные переформулировки**, которые BLEU пропускает.
- **BLEURT** (2020): делает принципиально новый шаг — **fine-tune BERT на датасете человеческих оценок**, обучая метрику **предсказывать** human judgment. Вместо hand-crafted правил (как в BLEU), метрика сама учится, что людям нравится.

Это был важный **сдвиг парадигмы**: от **hand-crafted правил** (BLEU: "считай совпадение 4-грамм, примени brevity penalty") к **learned models** — нейросетевым моделям, обученным на примерах человеческого мнения.

Однако **BLEURT всё ещё имеет критическое ограничение**: это — **небольшая модель**, обученная на **ограниченных данных**. Она не может:

- Адаптироваться к новым критериям (нужна переразметка и переобучение)
- Рассуждать и объяснять своё решение
- Обрабатывать многошаговые или сложные логические задачи

**BLEURT — это окончание пути для traditional metrics и neural scorers.**

**Эпоха 3: LLM-as-a-Judge — принципиально новая парадигма (2023 onwards)**

**Логичное следствие эволюции**: почему не использовать **саму большую языковую модель** в качестве судьи, вместо ограниченных нейросетей?

Это привело к появлению **LLM-as-a-Judge** (Zheng et al., 2023 MT-Bench):​

**Преимущества LLM-судей**:

- Мощь и понимание: полнофункциональная LLM может **рассуждать**, **объяснять** своё решение, **выявлять нюансы** контекста.
- Адаптивность: любые критерии задаются через **natural language prompts** — не требуется переобучение.
- Масштабируемость: работает через API, может обрабатывать **любое количество примеров**.
- Проверенная корректность: **80%+ согласия** с человеческими оценками на MT-Bench, Chatbot Arena и других бенчмарках (Zheng et al., 2023).

**Эволюция парадигмы оценки** (как показано в работе "The Rise of Agent-as-a-Judge", 2025):[](https://arxiv.org/html/2508.02994v1)
```
Traditional Automatic Metrics (BLEU, ROUGE, METEOR)
         ↓ [ограничения: поверхностность, отсутствие семантики]
Neural Metrics (BERTScore, BLEURT)
         ↓ [ограничения: fixed обучение, адаптация дорога]
Single-Model LLM Judges (GPT-4 as Judge)
         ↓ [направление: множество перспектив]
Multi-Agent Debate / Ensemble of Judges
         ↓ [направление: reasoning и tool use]
Agent-as-a-Judge (судья с инструментами, памятью, reasoning)
```

Дальнейшая эволюция: **Agent-as-a-Judge** может использовать инструменты (code execution, searches), память о контексте и многошаговое рассуждение. Результаты показывают, что такие судьи достигают **parity с человеческими оценками** — отличаются от human judgment всего на **0.3%** в некоторых задачах (тогда как single LLM judge отличался на **31%**).[](https://arxiv.org/html/2508.02994v1)​

Почему это всё важно:

1. **Неизбежность**: переход от метрик к LLM-судьям — это не capricious choice, а **логичный ответ на фундаментальные ограничения** предыдущих методов.
    
2. **Применимость к разметке**: если автоматические метрики не работают даже для оценки MT систем (где есть reference translation), они **ещё менее пригодны** для оценки разметки (где критерии часто субъективны и многомерны).
    
3. **LLM-судья как инструмент**: для твоей системы разметки, **LLM-судья** (критик) — это не просто novel idea, а **эволюционно продвинутое** решение, которое может:
    - Рассуждать о качестве аннотаций
    - Адаптироваться к любым критериям разметки
    - Масштабироваться к миллионам примеров
    - Объяснять свои решения (интерпретируемость)
4. **Конвергенция с другими идеями**: в контексте ансамблей исполнителей и эволюции инструкций, LLM-судья — это **естественный и проверенный** способ измерять качество работы ансамбля.
#### 2. Ансамбли LLM как естественное решение для преодоления плато качества

##### 2.1 Коллективный интеллект и теорема Кондорсе

История классической статистики и коллективного принятия решений давно показала мощь простого голосования большинства. Теорема Жюри Кондорсе (1785) и её современные развития утверждают фундаментальный принцип:

> *Если каждый судья независимо лучше случайного уровня и ошибки хотя бы частично независимы, то вероятность правильного решения большинства растет с числом судей и стремится к 1*

**В контексте LLM это означает, что несколько разных моделей, каждая со своими уклонами, знаниями и ошибками, при агрегировании могут дать результат, превосходящий любую отдельную модель**

##### 2.2 Практическое воплощение eLLM (Ensemble Large Lenguage Models)

Работа [[Статья Majority Rules - LLM Ensemble is a Winning Approach for Content Categorization]] является одной из самых убедительных эмпирических демонстраций этого принципа применительно к таксономической классификации. Подробнее про эксперименты можно почитать по ссылке, тут приведу результаты и выводы.

Имеем лучший результат одиночной модели в 0.55 F1, большинство других находятся в диапазоне 0.50-0.53, то есть вышли на плато одиночных моделей.

При этом результаты ансамбля:

- 2-модельный ансамбль (Gemini 2.0 + DeepSeek): F1 = 0.73 (+33 from best single)
- 3-модельный: F1 = 0.76
- 10-модельный ансамбль: F1 = 0.92 (+67% vs best single)

Эти цифры перемещают LLM разметку из зоны посредственного качества в зону приемлемого для производства качества и приблизительно до уровня человеческого эксперта по согласованности.

Механизм агрегирования: **Collective Decision-Making (CDM)**

Ключевой компонент - взвешенный консенсус через Relevance Score для каждого кандидата в категорию.

Для каждой категории *c* из объединенного набора всех предложенных разметчиками категорий вычисляется:

$$R(c)= \frac{N_c * L_c * Prox_c}{N * L_{max} } ⁡$$

где:
- $N_{c}$ - число моделей из ансамбля, которые выбрали категорию *с* (популярность)
- $N$ - общее число моделей в ансамбле
- $L_{c}$ - уровень категории в иерархии (более глубокие = более специфичные)
- $L_{max}$ - максимальная глубина иерархии (нормализация)
- $Prox_c$ - мера семантической близости категории к другим выбранным категориям (не должна быть одинокая в наборе)
- $τ$ - порог конценсуса - в финальный результат попадают только категории с $R(c) >= τ$

Оптимизация порога консенсуса:

Авторы провели sweep по значениям τ∈{0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95}:

- При τ=0.55: Recall = 1.00, но Precision = 0.69, F1 = 0.78 (слишком много ложных меток).
- При τ=0.65: Recall = 0.93, Precision = 0.94, F1 = 0.92 (оптимальный баланс).
- При τ=0.7: Precision = 1.00, но Recall = 0.72, F1 = 0.81 (слишком консервативный).

**Вывод**: оптимальный порог τ≈0.65τ≈0.65 обеспечивает наилучший баланс между точностью и полнотой, приблизительно соответствуя **человеческому уровню согласованности**.

Улучшение LLM-специфичных проблем:

- **Hallucination Ratio (HR)**: в ансамбле практически **полностью исчезают** галлюцинации (категории вне таксономии), так как хотя бы одна модель будет более дисциплинирована.
- **Category Inflation Ratio (IR)**: существенно снижается, выход становится более компактным и близким к человеческим аннотациям.

##### 2.3 Общая таксономия LLM-ансамблей: трёхмерная классификация

Обзор [[Статья LLM Ensemble - Harnessing Multiple Large Language Models - A Survey on LLM Ensemble]] предлагает фундаментальную классификацию всех подходов к ансамблированию LLM по трем осям, в зависимости от момента, когда происходит агрегирование

*(a) Ensemble-before-inference (ансамблирование ДО инференса)*

В этом подходе используется **router** (маршрутизатор) для выбора одной или нескольких подходящих моделей ещё до их вызова:

- **(a1) Pre-training router**: отдельная модель, обученная на логах запросов, где известно, какая LLM лучше справляется с каким типом задач. Плюсы: оптимальный выбор, экономия cost. Минусы: требует больший датасет для обучения router'а, требует maintenance.
- **(a2) Non-pre-training router**: эвристические правила или lightweight решения (например, «запросы длиной <100 слов → дешёвая модель, >500 слов → мощная модель»). Плюсы: простота. Минусы: грубый выбор.

*(b) Ensemble-during-inference (ансамблирование ВО ВРЕМЯ генерации)*

Здесь несколько моделей участвуют в процессе декодирования, агрегируя решения на разных уровнях гранулярности:

- **(b1) Token-level ensemble**: на каждом шаге генерации логиты/вероятности нескольких LLM агрегируются (усреднение, взвешивание) и из объединённого распределения семплируется следующий токен. Плюсы: максимально точная интеграция знаний. Минусы: требует доступа к logits (сложно с API), может быть вычислительно дорого.
- **(b2) Span-level ensemble**: разные модели генерируют кандидатные фрагменты (span) — например, несколько слов или предложение — а затем через ranking (perplexity или LLM-judge) выбирается лучший. Плюсы: хороший компромисс между точностью и вычислительной стоимостью. Минусы: требуется механизм ранжирования.
- **(b3) Process-level ensemble**: комбинирование цепочек рассуждений между несколькими агентами, особенно полезно для многошаговых задач (math, logic). Плюсы: лучше на задачах, где процесс решения важен. Минусы: сложность реализации и оценки качества промежуточных шагов.

*(c) Ensemble-after-inference (ансамблирование ПОСЛЕ генерации)*

Самый практичный и распространённый сценарий: каждая модель **полностью независимо** дает свой ответ, а затем результаты агрегируются:

- **(c1) Non-cascade methods**: все модели вызываются параллельно (или последовательно), затем применяется voting, ranking или CDM. **Это именно то, что делает eLLM**. Плюсы: не требует доступа к internals, легко реализуется над API. Минусы: cost (вызовы всех моделей).
- **(c2) Cascade methods**: цепочка моделей, отсортированная по cost/quality. Сначала вызывается дешёвая модель; если её ответ имеет высокую confidence или проходит качественный фильтр (LLM-judge), результат принимается. Иначе переходим к более мощной/дорогой модели. Плюсы: оптимизация cost без потери качества (большинство простых примеров обработано дешёво). Минусы: сложная логика управления, требуется хороший критерий остановки.

#### 3. LLM-as-a-judge: критик для управления и оценки разметки

##### 3.1 Мотивация и необходимость отдельного оценщика

В классическом crowdsourcing использовались такие методы ансамблирования:

- Simple voting - быстро, но игнорирует компетентность аннотаторов
- Dawid-Skene  - восстанавливает истинные метки и матрицы ошибок каждого аннотатора, но требует EM-алгоритм
- Эталонные (gold) примеры - случайно распределяют между аннотаторами для проверки; если аннотатор часто ошибается на GT, его вес снижается

Эти идеи опираются либо на мат. модели, либо на предварительно размеченные примеры, либо на простые мажоритарные правила

**Новая идея:** использовать мощный LLM в роли судьи-оценщика, который может:
- анализировать ответы исполнителей с контекстом задачи
- выдать мотивированное решение (не только метка, но и объяснение)
- адаптироваться к новым критериям через промптинг
- работать на open-ended задачах, где GT нет

##### 3.2 MT-Bench - LLM судья на замену человеку

Работа **«[[Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena]]»** (Zheng et al., 2023) была одной из первых показавших, что LLM (в частности GPT-4) может служить надежным заменителем человеческой оценки.

Экспериментальная схема:
- MT-bench - набор из 80 многошаговых вопросов (от простых до сложных)
- Chatbot Arena - краудсорсный сбор paiwise сравнений между ответами разных моделей на открытые вопросы
- GPT-4 как судья: для каждой пары ответов GPT-4 выбирал лучший и объяснял свой выбор

Результаты:

- 80% согласия между GPT-4 и человеческими оценками
- Корреляция с человеческими предпочтениями была сопоставима с inter-human agreement (т.е. люди также не всегда соглашаются друг с другом на 100%)
- Выявлены систематические смещения:
	- Position Bias - склонность выбирать второй ответ
	- Verbosity Bias - предпочтение более длинных ответов
	- Self-enhancement bias - модель завышает оценку свои собственных ответов

Вывод: сильные LLM смогут служить масштабируемым прокси для человеческих оценок, но нуждаются в калибровке и анализе смещений.

##### 3.3 Теоретическое обоснование: A survey om LLM-as-a-judge

Это обзор, который дает формальное определение и систематизацию всего поля LLM-судей.

Формальная модель:

**LLM-judge** - это система, которая выдает оценку E объекта *x* на основе контекста C (инструкций, примеров, критериев) через языковую модель: 
$$E←P_LLM(x⊕C)$$
где ⊕ - комбинирование входа и контекста в единый промпт.

Типы оценок (output форматы):
1. Скалярные баллы: от 1 до 10 и другие различные шкалы
2. Категориальные метки: бинарные, многоклассовые. Пример: "Является ли ответ релевантным?" -> {Да, Нет}
3. Pairwise предпочтенность: сравнение двух объектов, выбор лушего
4. Множественный выбор: выбор из списка возможных меток.

Источники улучшения судьи:

**Prompt design** (на основе in-context learning):
- Chain of Thought: "Сначала объясни своё рассуждение шаг за шагом, затем дай финальное решение." - показано, что такой подход значительно повышает точность и согласие с людьми
- **Few-shot примеры** - привести 2-3 примера с разбором. Судья учится на примере "как это должно выглядеть"
- **Role-Playing**: "представь, что ты эксперт по разметке текстов с опытом 30 лет". Помогает настроить стиль и требования судьи

**Model Selection** (выбор базовой LLM):

- **General LLM** (GPT-4, Claude): используются как есть, просто вызываются через API. Универсальны, но могут быть неоптимальны на специфичных задачах.
- **Fine-tuned LLM** (JudgeLM, Prometheus, PandaLM, Auto-J): модель fine-tune'ится (SFT) на датасет (примеры + эталонные оценки от GPT-4 или людей). Результат: 3–5x дешевле, 2–5% лучше на целевой задаче, но хуже на out-of-domain данных.

**Post-processing** (обработка вывода):

- **Voting между судьями**: вызвать несколько судей, их оценки агрегировать (majority vote, averaged scores).
- **Calibration**: применить температурное масштабирование или другие техники для лучшего calibration вероятностей.
- **Constrained decoding**: использовать грамматику (например, JSON schema) для гарантии правильного формата вывода.

Meta-evaluation (оценка самого судьи):

Судья оценивается через его **согласие с human labels**:

- **Классические метрики**: Accuracy (доля правильных оценок), Precision, Recall, F1-score.
- **Меры inter-rater agreement**:
    - **Cohen's kappa**: мера согласия для двух судей, исключающая случайное совпадение.
    - **Fleiss' kappa**: обобщение на несколько судей.
- **Analysis of bias**: проверка на систематические смещения (position bias, length bias и т.д.).
- **Robustness**: устойчивость к adversarial примерам, jailbreak попыткам, изменениям формулировки.

#### 4. Ограничения ручной оптимизации инструкций и необходимость автоматизации

##### 4.1 Почему ручная оптимизация инструкций неэффективна

Даже при ансамбле исполнителей и судье, остается критическая задача оптимизации инструкции для исполнителей (а может и для судьи тоже).

Ручная (expert-driven) оптимизация имеет ряд ограничений:

- **Субъективность и непостоянство:** разные эксперты пишут разные инструкции, часто с разным качеством. Один эксперт может использовать одни подходы промптинга и пренебречь другими. Невозможно предсказать какой стиль лучше для конкретного набора модели И ДЛЯ КОНКРЕТНОЙ ЗАДАЧИ
- **Не масштабируемость:** проверка каждого варианта инструкции требует обучения, вычислений, анализа результатов. Для сложных задач это часы, дни, недели
- **Локальные оптимумы:** эксперты обычно, начав с некоторой инструкции, вносят небольшие изменения, улучшающие качество на небольшую величину. Это может привести к застреванию в локальном оптимуме, когда радикально другая инструкция была бы гораздо лучше
- **Неопределенность в многокритериальной оптимизации:** инструкция часто должно одновременно следовать нескольким критериям:
	- максимизировать точность (accuracy)
	- максимизировать согласие между исполнителями (inter-annotator agreement)
	- минимизировать затраты
	- быть понятной исполнителям (не слишком сложной)
	Ручной trade-off между этими критериями - это угадывание, а не оптимизация

##### 4.2 AlphaEvolve как автоматический оптимизатор инструкций

**AlphaEvolve** (Google DeepMind, 2025) — это система для автоматического поиска оптимальных дискретных объектов (программы, инструкции, конфигурации) через эволюционное программирование с использованием LLM в качестве sampler'а.

Архитектура OpenEvolve (открытой реализации):

**Основные компоненты**:

1. **Controller**: управляет жизненным циклом эволюции (инициализация, выбор лучших, генерация новых кандидатов, завершение).
2. **Prompt Sampler** (генератор вариантов):
    - Получает: best инструкции из базы данных, вдохновляющие примеры (разнообразные), текущие метрики.
    - Генерирует: новые варианты инструкций через LLM (как правило, мощная модель типа GPT-4).
    - Стратегии мутации:
        - коллаборативная фильтрация (какие элементы best инструкций комбинировать?);
        - gradient-free оптимизация (пробовать "соседей" в пространстве инструкций);
        - LLM-directed синтез (попросить LLM улучшить инструкцию, заметив её слабые стороны).
3. **LLM Ensemble** (исполнители):
    - Несколько моделей применяют текущую инструкцию к примерам из обучающей выборки.
    - Выбор моделей зависит от целей (дешевизна, разнообразие, мощность).
4. **Evaluator** (оценщик):
    - Берёт ответы исполнителей на примерах и вычисляет метрики качества.
    - Метрики могут быть:
        - автоматические (accuracy vs gold labels, если они есть);
        - судей-ориентированные (согласие с LLM-judge, consensus score);
        - комбинированные (weighted sum нескольких метрик).
5. **Database** (хранилище):
    - **MAP-Elites** архитектура: вместо одной "лучшей" инструкции хранится **популяция** инструкций, оптимальных по разным критериям.
    - Оси (dimensions): например, (Accuracy, Cost) или (Consensus, Brevity).
    - Миграция между островами: обмен хорошими инструкциями между параллельными worker'ами.
    - Результат: набор Pareto-оптимальных инструкций, из которых эксперт может выбрать нужный компромисс.

Процесс эволюции (итерация):

1. Инициализация: случайные или expert-written инструкции.
2. Оценка: каждая инструкция применяется к выборке, вычисляются метрики.
3. Отбор: лучшие инструкции по всем критериям копируются в следующее поколение.
4. Вариация: sampler создаёт варианты (мутации) лучших инструкций.
5. Повтор (2–4) на несколько сотен итераций.

 Преимущества эволюционного подхода:

- **Градиент-free**: работает в дискретном пространстве инструкций (нет гладкого градиента).
- **Разнообразие**: MAP-Elites находит несколько разных хороших решений, а не одно.
- **Адаптивность**: процесс адаптируется к выбранным метрикам (точность, cost, consensus — всё одновременно).
- **Масштабируемость**: можно распараллелить (разные workers ищут инструкции параллельно, периодически обмениваясь лучшими).
    

 Почему AlphaEvolve идеален для разметки:
 
- **Дискретная оптимизация**: инструкции — это текст, не дифференцируемый. Традиционные gradient-based методы неприменимы. Эволюция — естественный выбор.
- **Множество метрик**: можно одновременно оптимизировать F1, inter-annotator agreement, cost, читаемость инструкции.
- **Robustness**: эволюция исследует большое разнообразие инструкций, находя те, которые работают "вообще", а не на одном примере.


#### 5. Синтез N исполнителей + Критик + Эволюция инструкций

> **Есть N исполнителей, есть один критик. Ему нужно подобрать инструкцию так, чтобы N исполнителей решили задачу с приемлемым качеством.**

Эта формулировка являет собой **идеальный синтез трёх независимых направлений исследований**, каждое из которых доказало свою эффективность:

1. **N исполнителей** — коллективный интеллект и ансамбли LLM (eLLM, идеи Condorcet).
2. **Один критик** — LLM-as-a-Judge для оценки и управления процессом разметки.
3. **Подобрать инструкцию** — автоматическая эволюционная оптимизация через OpenEvolve/AlphaEvolve.

##### 5.1 Комбинируем 3 направления

Хотя каждое из трёх компонентов (ансамбли LLM, LLM-судьи, эволюция инструкций) интенсивно изучаются отдельно, **синергетическое комбинирование** их в единую систему для **автоматической оптимизации процесса разметки данных** является **новым и высоко актуальным направлением**:

- **eLLM** (Kamen, 2025) показала мощь ансамблей для классификации с CDM агрегацией, но **не рассматривала эволюцию инструкций** как способ дальше улучшать качество.
- **LLM-as-a-Judge** интенсивно обсуждается в контексте **оценки ответов моделей** (MT-Bench, Chatbot Arena, обзоры Gu и др.), но **не в контексте активного управления процессом разметки** и эволюции инструкций через feedback от судьи.
- **AlphaEvolve** в своей оригинальной публикации сосредоточена на **кодогенерации и научном открытии** (синтез алгоритмов), а **не на разметке данных как primary domain**.
    

**Работа** заполняет этот пробел: создание и исследование **интегрированной системы**, в которой:

1. Множество исполнителей (LLM-аннотаторов) работают в ансамбле.
2. LLM-судья оценивает качество их совместной работы по заданным критериям.
3. OpenEvolve систематически ищет оптимальные инструкции для исполнителей, которые максимизируют согласие судьи и метрики качества.




